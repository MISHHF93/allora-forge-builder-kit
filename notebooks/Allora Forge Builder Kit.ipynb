{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecaec0fe-434c-4874-9281-becf2798dc73",
   "metadata": {
    "id": "ecaec0fe-434c-4874-9281-becf2798dc73"
   },
   "source": [
    "# Allora Forge Builder Kit\n",
    "\n",
    "This notebook with help you get started submitting inferences to the Allora network and take part in the Forge competition.\n",
    "\n",
    "## In this notebook you will quickly be able to:\n",
    "\n",
    "  1. Load training and validation data.\n",
    "  2. Load a LightGBM model\n",
    "  3. Train the model\n",
    "  4. Evaluate the model on validation data (recent 6 months)\n",
    "  5. Package your predict function into a `.pkl` file\n",
    "  6. Run your worker node to deliver inferences to Allora's network in real time\n",
    "\n",
    "## What you will need\n",
    "\n",
    "- **Allora Data API Key** (get your API key here: [https://developer.allora.network/](https://developer.allora.network/))\n",
    "\n",
    "## Important: Allora Wallets\n",
    "\n",
    "The last cell of this notebook runs a **worker** node, whose job it is to provide inferences to the Allora network. It is part of [Allora's Python SDK](https://github.com/allora-network/allora-sdk-py) and installs with `pip install allora-sdk`.\n",
    "\n",
    "Code Example\n",
    "```\n",
    "from allora_sdk.worker import AlloraWorker\n",
    "\n",
    "worker = AlloraWorker(\n",
    "    predict_fn=predict_fn,\n",
    "    api_key=api_key,\n",
    ")\n",
    "```\n",
    "\n",
    "The worker automatically handles wallets. If you don't have a wallet, just hist <ENTER> when prompted for your menmonic phrase, and **a new wallet will be created automatically** and stored in the file marked `.allora_key`. If the key file exists, the worker will just run using that file. If you wish to enter a new mnemonic pass phrase, rename the `.allora_key` file to something else, the system will promopt you again for a new menmonic pass phrase.\n",
    "\n",
    "Your wallet's inportant attributes are\n",
    "\n",
    " 1. The Address\n",
    " 2. The Menmonic Phrase (secret)\n",
    "\n",
    "The address is public. It identifies your worker on Allora's network. Browse the network with Allora's [network explorer](https://explorer.allora.network/) to find your wallet address. Never tell anyone your mnemonic pass phrase, only use it to run your worker and interact with the blockchain.\n",
    "\n",
    "**Important**: Keep track of your wallet details. The address is what you will use to participate in the Forge competitions and get access to your funds later.\n",
    "\n",
    "# Happy Coding\n",
    "\n",
    "Let's go."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8590579",
   "metadata": {
    "id": "a8590579",
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Install all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06249ff0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "06249ff0",
    "outputId": "641b9a60-416e-4dfc-e575-c0e0b29ec9b8"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Install all necessary packages\n",
    "'''\n",
    "\n",
    "%pip install git+https://github.com/allora-network/allora-forge-builder-kit.git\n",
    "%pip install allora_sdk>=1.0.5 lightgbm scikit-learn pandas numpy matplotlib dill cloudpickle\n",
    "\n",
    "# Optional display-only sanity echo\n",
    "import os\n",
    "if os.getenv(\"ALLORA_WALLET_ADDR\"):\n",
    "    print(f\"Sanity (display only): ALLORA_WALLET_ADDR={os.getenv('ALLORA_WALLET_ADDR')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ff6db6",
   "metadata": {
    "id": "d1ff6db6"
   },
   "source": [
    "## Imports\n",
    "\n",
    "The following libraries are required for this workflow:\n",
    "- `allora_forge_builder_kit` for interfacing with the Allora Forge ML Workflow.\n",
    "- `allora_sdk` for communicating with the Allora network\n",
    "- `lightgbm` for model training.\n",
    "- `pandas` and `numpy` for data manipulation.\n",
    "- `cloudpickle` for serialization.\n",
    "- `matplotlib` for visualization.\n",
    "- `time` for timing operations.\n",
    "\n",
    "These will be imported in the next code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c374ee-d3ca-4b9a-8a01-9719bf042361",
   "metadata": {
    "id": "24c374ee-d3ca-4b9a-8a01-9719bf042361"
   },
   "outputs": [],
   "source": [
    "from allora_forge_builder_kit import AlloraMLWorkflow, get_api_key\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import cloudpickle\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6e677a",
   "metadata": {
    "id": "af6e677a"
   },
   "source": [
    "## API KEY\n",
    "This gives you access to OHLCV (open, high, low, close, volume) candle data through the workflow.\n",
    "\n",
    "To get your API key, go to [https://developer.allora.network/](https://developer.allora.network/), create an account, and generate a new API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b421eb-019f-4307-b574-68cf64d7c532",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "09b421eb-019f-4307-b574-68cf64d7c532",
    "outputId": "50f24ddf-0b22-47c8-9955-c453dfed69fd"
   },
   "outputs": [],
   "source": [
    "api_key = get_api_key()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc8eb90",
   "metadata": {
    "id": "5fc8eb90"
   },
   "source": [
    "## Initializing the Workflow\n",
    "### Defining Assets, Input Feature Window, Target\n",
    "\n",
    "Before training a model, we need to specify how much historical data to use as input features, and how far into the future we want to predict (the target). In this section, we define the parameters that control the *shape* and *amount* of our input features, which are simply normalized historical OHLCV candles.\n",
    "\n",
    "- **tickers**: The set of tickers for data.\n",
    "- **hours_needed**: This sets the total length (in hours) of the lookback window, i.e., how much historical context the model will see for each prediction.\n",
    "- **number_of_input_candles**: This determines how many candles (or chunks) the lookback window is divided into. Each candle summarizes a portion of the lookback window, and together they form the input features.\n",
    "- **target_length**: This specifies how many hours into the future we want to predict. For the Allora Forge competition, this should match the target horizon required by the challenge.\n",
    "\n",
    "For example:\n",
    "\n",
    "- Setting `hours_needed = 72` and `number_of_input_candles = 3` gives **3 daily candles** as input (each candle covers 24 hours).\n",
    "- Setting `hours_needed = 24` and `number_of_input_candles = 24` gives **24 hourly candles** as input (each candle covers 1 hour).\n",
    "\n",
    "We'll visualize the resulting features a few cells below, so you can see exactly what the model will receive as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84a3fab-4dc4-4173-b5e1-6f5e863fd226",
   "metadata": {
    "id": "d84a3fab-4dc4-4173-b5e1-6f5e863fd226"
   },
   "outputs": [],
   "source": [
    "ticker = \"btcusd\" #eg btcusd, ethusd, solusd\n",
    "tickers = [ticker] # for multiple tickers: ['btcusd', 'ethusd', 'solusd']\n",
    "hours_needed = 5*24             # Number of historical hours for feature lookback window\n",
    "number_of_input_candles = 12    # Number of candles for input features\n",
    "target_length = 24            # Number of hours into the future for target (depends on forge competition)\n",
    "\n",
    "# Instantiate the workflow\n",
    "workflow = AlloraMLWorkflow(\n",
    "    data_api_key=api_key,\n",
    "    tickers=tickers,\n",
    "    hours_needed=hours_needed,\n",
    "    number_of_input_candles=number_of_input_candles,\n",
    "    target_length=target_length\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e68de6",
   "metadata": {
    "id": "50e68de6"
   },
   "source": [
    "## Getting Data\n",
    "### Training, Validation, and Test Data for Supervised Learning\n",
    "\n",
    "The `get_train_validation_test_data` function splits your historical data into three consecutive time periods: **training**, **validation**, and **test**. This is a standard approach in supervised machine learning to ensure that your model is evaluated on data it has never seen during training.\n",
    "\n",
    "### Arguments:\n",
    "- **`from_month`**: The starting month (YYYY-MM) for the entire dataset. The earliest available is `\"2020-01\"`.\n",
    "- **`validation_months`**: Number of months to reserve for the validation set (used for tuning hyperparameters and early stopping).\n",
    "- **`test_months`**: Number of months to reserve for the test set (used for final evaluation).\n",
    "\n",
    "### Data Splits:\n",
    "The data is split chronologically:\n",
    "1. **Training Set**: Includes all data from `from_month` up to the start of the validation period.\n",
    "2. **Validation Set**: Immediately follows the training set, with an embargo (gap) equal to the prediction target length (e.g., 24 hours) to prevent data leakage.\n",
    "3. **Test Set**: Follows the validation set, again separated by an embargo of the same length.\n",
    "\n",
    "### How Long Does it Take?\n",
    "Loading data should only take a few minutes, and versions of data are cached for quicker load times in the future.\n",
    "- Force a re-download of fresh data with `force_redownload=True`.\n",
    "\n",
    "### Visual Schematic:\n",
    "For a 24-hour target:\n",
    "\n",
    "| **Training Set** | → **24 hr** → | **Validation Set** | → **24 hr** → | **Test Set** |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7babacab-f6cf-4559-957e-279ae0ae4305",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7babacab-f6cf-4559-957e-279ae0ae4305",
    "outputId": "494f1b7b-cd57-48ac-a14b-7e77aa46bd19"
   },
   "outputs": [],
   "source": [
    "# Get training, validation, and test data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = workflow.get_train_validation_test_data(\n",
    "    from_month=\"2023-01\",\n",
    "    validation_months=3,\n",
    "    test_months=3,\n",
    "    force_redownload=False  # Set to True to force re-download of fresh data\n",
    ")\n",
    "\n",
    "# Example: Check the shapes of the datasets\n",
    "print(\"Training set:\", X_train.shape, y_train.shape)\n",
    "print(\"Validation set:\", X_val.shape, y_val.shape)\n",
    "print(\"Test set:\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b221f8",
   "metadata": {
    "id": "49b221f8"
   },
   "source": [
    "## Visualizing Data Time Spans\n",
    "\n",
    "The next cell visualizes the time spans covered by the training, validation, and test datasets. This helps to ensure that the data splits are correctly separated in time and that there is no data leakage between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a656fd3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "id": "2a656fd3",
    "outputId": "1d59c7eb-5e13-476d-cb5f-ac87f3c1abfc"
   },
   "outputs": [],
   "source": [
    "# Print start and end dates for each split\n",
    "def print_split_dates(X, name):\n",
    "    if isinstance(X.index, pd.MultiIndex):\n",
    "        dates = X.index.get_level_values(0)\n",
    "    else:\n",
    "        dates = X.index\n",
    "    print(f\"{name} set: {dates.min()} to {dates.max()}\")\n",
    "\n",
    "print_split_dates(X_train, \"Training\")\n",
    "print_split_dates(X_val, \"Validation\")\n",
    "print_split_dates(X_test, \"Test\")\n",
    "\n",
    "# Visualize the splits on a timeline\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 2))\n",
    "\n",
    "def plot_span(X, color, label):\n",
    "    if isinstance(X.index, pd.MultiIndex):\n",
    "        dates = X.index.get_level_values(0)\n",
    "    else:\n",
    "        dates = X.index\n",
    "    ax.axvspan(dates.min(), dates.max(), color=color, alpha=0.4, label=label)\n",
    "\n",
    "plot_span(X_train, 'tab:blue', 'Training')\n",
    "plot_span(X_val, 'tab:orange', 'Validation')\n",
    "plot_span(X_test, 'tab:green', 'Test')\n",
    "\n",
    "ax.legend(loc='upper left')\n",
    "ax.set_title(\"Train/Validation/Test Splits Timeline\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "ax.xaxis.set_major_formatter(mdates.ConciseDateFormatter(ax.xaxis.get_major_locator()))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd389fb",
   "metadata": {
    "id": "ccd389fb"
   },
   "source": [
    "## Visualizing Features and Historical Price Process\n",
    "\n",
    "In the next cell, we select a single row of data and visualize the features associated with it. These features represent the normalized historical OHLCV (open, high, low, close, volume) candles leading up to the selected time index.\n",
    "\n",
    "To provide additional context, we overlay the historical price process during the same time period. This allows us to visually compare the input features with the actual price movements of the asset, offering insights into the relationship between the features and the price process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe9faa0-7af6-41fc-8553-bb51caccf03d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 588
    },
    "id": "3fe9faa0-7af6-41fc-8553-bb51caccf03d",
    "outputId": "53068d7a-2bf1-425b-eae7-2f3bf5b75a8e"
   },
   "outputs": [],
   "source": [
    "this_data = X_train.loc[(slice(None), 'btcusd'), :]\n",
    "\n",
    "data_idx = -10000  # which data point to visualize\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot features based on data_idx\n",
    "ax1.plot([this_data.iloc[data_idx][f\"feature_open_{i}\"] for i in range(number_of_input_candles)], 'o', label=\"open\")\n",
    "ax1.plot([this_data.iloc[data_idx][f\"feature_high_{i}\"] for i in range(number_of_input_candles)], 'o', label=\"high\")\n",
    "ax1.plot([this_data.iloc[data_idx][f\"feature_low_{i}\"] for i in range(number_of_input_candles)], 'o', label=\"low\")\n",
    "ax1.plot([this_data.iloc[data_idx][f\"feature_close_{i}\"] for i in range(number_of_input_candles)], 'o', label=\"close\")\n",
    "ax1.set_xlabel(\"Candle Index\")\n",
    "ax1.set_ylabel(\"Normalized Price Features\")\n",
    "ax1.legend(loc=\"upper left\")\n",
    "\n",
    "# Plot past price process based on data_idx\n",
    "plt.twiny()\n",
    "plt.plot(\n",
    "    (this_data.iloc[data_idx - (hours_needed * 12):data_idx]['close'].values) / this_data.iloc[data_idx]['close'],\n",
    "    label=\"5 min close (past)\"\n",
    ")\n",
    "plt.legend(loc='center left')\n",
    "\n",
    "# Plot volume as a bar chart with independent y-axis\n",
    "ax2 = ax1.twinx()\n",
    "ax2.bar(range(number_of_input_candles),\n",
    "    [this_data.iloc[data_idx][f\"feature_volume_{i}\"] for i in range(number_of_input_candles)],\n",
    "    alpha=0.3, color='gray', label=\"volume\")\n",
    "ax2.set_ylabel(\"Normalized Volume\")\n",
    "ax2.legend(loc=\"upper right\")\n",
    "\n",
    "plt.title(\"Features and Volume\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b8b7d2",
   "metadata": {
    "id": "85b8b7d2"
   },
   "source": [
    "## Visualizing the Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8645f3ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "8645f3ad",
    "outputId": "c6cdfbbd-0101-42d9-d378-935c4f654816"
   },
   "outputs": [],
   "source": [
    "this_data = X_train.loc[(slice(None), 'btcusd'), :]\n",
    "\n",
    "data_idx = 80000  # which data point to visualize\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Historical price process (normalized so last value is 1)\n",
    "hist_prices = this_data.iloc[data_idx - (hours_needed * 12):data_idx]['close']\n",
    "hist_times = this_data.iloc[data_idx - (hours_needed * 12):data_idx].index.get_level_values(0)\n",
    "if not hist_prices.empty:\n",
    "    hist_norm = hist_prices / hist_prices.iloc[-1]\n",
    "else:\n",
    "    hist_norm = hist_prices\n",
    "\n",
    "# Future price process (normalized so first value is 1)\n",
    "selected_time = this_data.iloc[data_idx].name[0] if isinstance(this_data.index, pd.MultiIndex) else this_data.index[data_idx]\n",
    "future_time = selected_time + pd.Timedelta(hours=target_length)\n",
    "future_prices = this_data['close']\n",
    "future_mask = (future_prices.index.get_level_values(0) > selected_time) & \\\n",
    "              (future_prices.index.get_level_values(0) <= future_time)\n",
    "future_price_series = future_prices[future_mask]\n",
    "if not future_price_series.empty:\n",
    "    future_norm = future_price_series / future_price_series.iloc[0]\n",
    "else:\n",
    "    future_norm = future_price_series\n",
    "\n",
    "# Plot historical and future price process on the same axis\n",
    "ax.plot(hist_times, hist_norm.values, label=\"5 min close (past)\", color='blue')\n",
    "ax.plot(future_price_series.index.get_level_values(0), future_norm.values, label=\"5 min close (future)\", color='green')\n",
    "ax.axhline(1, color='gray', linestyle='--', label='Transition Point')\n",
    "ax.set_xlabel(\"Time\")\n",
    "ax.legend(loc='center left')\n",
    "\n",
    "# Overlay target value\n",
    "target_value = y_train.loc[(selected_time, 'btcusd')] if isinstance(y_train.index, pd.MultiIndex) else y_train[selected_time]\n",
    "ax.scatter([future_time], [1 + target_value], color='red', label='Target Value', zorder=5)\n",
    "ax.legend(loc='upper left')\n",
    "\n",
    "plt.title(\"Historical & Future Price Process, and Target Value\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ba097c",
   "metadata": {
    "id": "e0ba097c"
   },
   "source": [
    "## \"Hello World!\" ML Model\n",
    "\n",
    "- This notebook demonstrates a simple **LightGBM model** that predicts the future price of an asset based on its past price process, as represented by the features.\n",
    "- The **validation set** is used to:\n",
    "    - Tune the model.\n",
    "    - Perform **early stopping**, which helps prevent overfitting.\n",
    "- The **test set** is used to evaluate the model's performance.\n",
    "\n",
    "### Workflow Summary:\n",
    "1. The model is trained on the **training set**.\n",
    "2. Early stopping is applied using the **validation set** to determine the best iteration.\n",
    "3. The model is retrained on the combined **training and validation data** using the best iteration found during training.\n",
    "4. Finally, the model is evaluated on the **test set**.\n",
    "\n",
    "---\n",
    "\n",
    "### Future Directions:\n",
    "- This is just one example of a modeling workflow.\n",
    "- This is where a lot of **modeling magic** can happen:\n",
    "    - Experiment with swapping in new techniques and models.\n",
    "    - Try different feature engineering strategies.\n",
    "    - Explore alternative evaluation metrics.\n",
    "- Stay tuned! More notebooks will be available in the future with **different ideas and approaches** to inspire your modeling journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973a8646-10e1-49ef-ba2c-19035d83eca1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "973a8646-10e1-49ef-ba2c-19035d83eca1",
    "outputId": "caefdb7a-ac9a-4ed8-ad69-104e7074a02e"
   },
   "outputs": [],
   "source": [
    "#define feature columns and ML model\n",
    "feature_cols = [f for f in list(X_train) if 'feature' in f]\n",
    "\n",
    "#define hyperparameters for the LightGBM model\n",
    "learning_rate = 0.1\n",
    "max_depth = 5\n",
    "num_leaves = 16\n",
    "\n",
    "# Initialize LightGBM model with hyperparameters\n",
    "model = lgb.LGBMRegressor(\n",
    "    n_estimators=2000,  # Set a high number of estimators for early stopping\n",
    "    learning_rate=learning_rate,\n",
    "    max_depth=max_depth,\n",
    "    num_leaves=num_leaves\n",
    ")\n",
    "\n",
    "# Define custom evaluation metric for correlation\n",
    "def corr_eval_metric(y_true, y_pred):\n",
    "    corr = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "    # LightGBM expects: (eval_name, eval_result, is_higher_better)\n",
    "    return 'corr', corr, True\n",
    "\n",
    "# Fit model with early stopping\n",
    "model.fit(\n",
    "    X_train[feature_cols], y_train,\n",
    "    # eval_set=[(X_val[feature_cols], y_val)],\n",
    "    # eval_metric=corr_eval_metric,\n",
    "    # callbacks=[\n",
    "    #     lgb.early_stopping(stopping_rounds=100, verbose=True)\n",
    "    # ]\n",
    ")\n",
    "# best_iteration = model.best_iteration_\n",
    "\n",
    "# #retrain the model to best iteration on train + validation data\n",
    "# model = lgb.LGBMRegressor(\n",
    "#     n_estimators=best_iteration,  # Use the optimal number of trees\n",
    "#     learning_rate=learning_rate,\n",
    "#     max_depth=max_depth,\n",
    "#     num_leaves=num_leaves\n",
    "# )\n",
    "model.fit(\n",
    "    pd.concat([X_train[feature_cols], X_val[feature_cols]]),\n",
    "    pd.concat([y_train, y_val])\n",
    ")\n",
    "# Evaluate\n",
    "#  on the test data\n",
    "test_preds = model.predict(X_test[feature_cols])\n",
    "test_preds = pd.Series(test_preds, index=X_test.index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53d1549",
   "metadata": {
    "id": "e53d1549"
   },
   "source": [
    "## The Workflow has a built-in method to evaluate the test data predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189af7a2-678a-4c41-91b7-e11cc305382d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "189af7a2-678a-4c41-91b7-e11cc305382d",
    "outputId": "8a284865-53a4-4cf7-f3a2-ac9eb49abf62"
   },
   "outputs": [],
   "source": [
    "#show test metrics\n",
    "metrics = workflow.evaluate_test_data(test_preds)\n",
    "print(metrics)\n",
    "\n",
    "#check directional accuracy for only predictions over a certain threshold, like 0.01\n",
    "threshold = .02\n",
    "good_data_index = np.abs(test_preds) > threshold\n",
    "good_preds = test_preds[good_data_index]\n",
    "good_trues = y_test[good_data_index]\n",
    "da = np.mean(np.sign(good_preds) == np.sign(good_trues))\n",
    "print(da)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8e5a38",
   "metadata": {
    "id": "5a8e5a38"
   },
   "source": [
    "## Retraining for live deployment\n",
    "The model is finally re-trained on the entire dataset, including the test data,\n",
    "before it is packaged into a pickled function for live predictions.\n",
    "\n",
    "The `workflow.get_live_features` function creates a version of the features based on the most up-to-date live data, in exactly the same way that the historical data features were created, ensuring the model is getting coherent live data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470116dc-3269-4d1b-b3ee-fd24af532ebf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "470116dc-3269-4d1b-b3ee-fd24af532ebf",
    "outputId": "67f38b12-4011-4e96-fb8f-d609e644179a"
   },
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    pd.concat([X_train[feature_cols], X_val[feature_cols], X_test[feature_cols]]),\n",
    "    pd.concat([y_train, y_val, y_test])\n",
    ")\n",
    "\n",
    "# Final predict function\n",
    "def predict() -> pd.Series:\n",
    "    live_features = workflow.get_live_features(\"btcusd\")\n",
    "    preds = model.predict(live_features)\n",
    "    return float( np.array( preds ) [0] )\n",
    "\n",
    "# Pickle the function\n",
    "with open(\"predict.pkl\", \"wb\") as f:\n",
    "    cloudpickle.dump(predict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f9e44a",
   "metadata": {
    "id": "67f9e44a"
   },
   "source": [
    "## Test Live Predictions\n",
    "\n",
    "This function simulates a live prediction scenario by loading the pickled function,\n",
    "calling it, and printing the prediction time and result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4f431e-e3c2-4e29-b3dd-4a2434aa6865",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8d4f431e-e3c2-4e29-b3dd-4a2434aa6865",
    "outputId": "76156db4-1f8c-402c-f9dd-4bba62494f42"
   },
   "outputs": [],
   "source": [
    "# Load the pickled predict function\n",
    "with open(\"predict.pkl\", \"rb\") as f:\n",
    "    predict_fn = cloudpickle.load(f)\n",
    "\n",
    "# Call the function and get predictions\n",
    "tic = time.time()\n",
    "prediction = predict_fn()\n",
    "toc = time.time()\n",
    "\n",
    "print(\"predict time: \", (toc - tic) )\n",
    "print(\"prediction: \", prediction )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897c2d72",
   "metadata": {
    "id": "897c2d72"
   },
   "source": [
    "## 🚀 Run Your Worker Node\n",
    "\n",
    "The following code loads your pickled prediction function and submits predictions to the Allora network at the required intervals.\n",
    "\n",
    "When you are asked for your mnemonic, you can simply hit &lt;ENTER&gt; and the worker will generate one automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028bfa68",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "028bfa68",
    "outputId": "bda96556-3ba8-4b03-c8b8-1d5f69d9703e"
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from allora_sdk.worker import AlloraWorker\n",
    "\n",
    "async def main():\n",
    "    with open(\"predict.pkl\", \"rb\") as f:\n",
    "        predict_fn = cloudpickle.load(f)\n",
    "\n",
    "    worker = AlloraWorker(\n",
    "        predict_fn=predict_fn,\n",
    "        api_key=api_key,\n",
    "    )\n",
    "\n",
    "    async for result in worker.run():\n",
    "        if isinstance(result, Exception):\n",
    "            print(f\"error: {str(result)}\")\n",
    "        else:\n",
    "            print(\"Prediction submitted to Allora.\")\n",
    "\n",
    "await main()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
