#!/usr/bin/env python3
"""
BTC/USD 7-Day Log-Return Prediction Submission Daemon - REFACTORED
-------------------------------------------------------------------
ENHANCED VERSION WITH:
- Official Allora docs RPC endpoints (Ankr recommended)
- Robust RPC endpoint failover with detailed diagnostics
- Comprehensive CSV logging with on-chain verification
- Explicit nonce/sequence mismatch handling
- Hourly heartbeat and health reporting
- Allora API fallback for repeated RPC failures
- Never-silent-fail guarantee with full error tracking

Based on: https://docs.allora.network/devs/consumers/rpc-data-access

DAEMON MODE: Runs as a reliable long-lived process until December 15, 2025
- Catches ALL exceptions and logs full tracebacks
- Hourly heartbeat/liveness check
- Never silently fails
- Suitable for systemd/supervisord auto-restart
- Validates model on every cycle

Usage:
    python submit_prediction_refactored.py [--model MODEL_PATH] [--features FEATURES_PATH] [--topic-id TOPIC_ID] [--dry-run] [--once]
    python submit_prediction_refactored.py --daemon  (run as permanent daemon)
"""

import os
import sys
import json
import shutil
import logging
import logging.handlers
import argparse
import subprocess
import hashlib
import base64
import time
import math
import signal
import traceback
from typing import List, Optional, Tuple
from datetime import datetime, timezone

from dotenv import load_dotenv
load_dotenv()

# Official RPC Endpoints from Allora docs
# https://docs.allora.network/devs/consumers/rpc-data-access
RPC_ENDPOINTS = [
    {"url": "https://rpc.ankr.com/allora_testnet", "name": "Ankr (Official Recommended)", "priority": 1, "type": "REST"},
    {"url": "https://allora-rpc.testnet.allora.network/", "name": "Allora Official", "priority": 2, "type": "REST"},
    {"url": "https://allora-testnet-rpc.allthatnode.com:1317/", "name": "AllThatNode", "priority": 3, "type": "REST"},
    {"url": "https://allora.api.chandrastation.com/", "name": "ChandraStation", "priority": 4, "type": "REST"},
]

# Allora API Fallback (for when RPC endpoints repeatedly fail)
ALLORA_API_BASE = os.getenv("ALLORA_API_BASE", "https://api.testnet.allora.network")
ALLORA_API_ENABLED = os.getenv("ALLORA_API_FALLBACK", "true").lower() == "true"

# Global state for RPC endpoint rotation with comprehensive tracking
_rpc_endpoint_index = 0
_failed_rpc_endpoints = {}  # endpoint_url -> {count, last_error, timestamp, errors_list}
_rpc_endpoint_last_used = None
_rpc_endpoint_success_count = {}  # endpoint_url -> success_count (for rotation priority)
_submission_attempt_count = 0
_max_submission_retries = 3
_last_rpc_report_time = None  # For hourly RPC health reports
_last_heartbeat_hour = None  # For tracking hourly heartbeats

import requests
import numpy as np
import pandas as pd

from allora_sdk import LocalWallet, AlloraRPCClient
from allora_sdk.protos.emissions.v9 import InputWorkerDataBundle, InputInferenceForecastBundle, InputInference, InsertWorkerPayloadRequest

# Simple Nonce class since import fails
class Nonce:
    def __init__(self, block_height: int):
        self.block_height = block_height

###############################################################################
# Response Validation - Detect Invalid JSON/HTML responses
###############################################################################
def validate_json_response(response_text: str, context: str = "") -> tuple[bool, dict]:
    """Validate that response is valid JSON, not HTML error page."""
    response_text = response_text.strip()
    
    # Check for HTML responses (error pages)
    if response_text.startswith("<"):
        logger.debug(f"HTML response detected {context}: {response_text[:100]}")
        return False, {}
    
    # Check for empty response
    if not response_text:
        logger.debug(f"Empty response received {context}")
        return False, {}
    
    # Try to parse JSON
    try:
        data = json.loads(response_text)
        return True, data
    except json.JSONDecodeError as e:
        logger.debug(f"Invalid JSON response {context}: {e}")
        return False, {}

###############################################################################
# Logging Setup - Enhanced for Daemon
###############################################################################
def setup_logging(log_file: str = "logs/submission.log"):
    """Configure logging with both console and file output."""
    logger = logging.getLogger("btc_submit")
    logger.setLevel(logging.DEBUG)
    
    # Create logs directory if needed
    log_dir = os.path.dirname(log_file)
    if log_dir and not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    # Console handler (INFO level)
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    console_fmt = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%SZ'
    )
    console_handler.setFormatter(console_fmt)
    
    # File handler with rotation (DEBUG level - capture everything)
    file_handler = logging.handlers.RotatingFileHandler(
        log_file,
        maxBytes=50*1024*1024,  # 50 MB
        backupCount=5,           # Keep 5 rotated files
        encoding='utf-8'
    )
    file_handler.setLevel(logging.DEBUG)
    file_fmt = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - [%(funcName)s:%(lineno)d] - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%SZ'
    )
    file_handler.setFormatter(file_fmt)
    
    # Add handlers
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    
    return logger

logger = setup_logging()

# Global state for daemon
_shutdown_requested = False

def signal_handler(signum, frame):
    """Handle termination signals gracefully."""
    global _shutdown_requested
    signal_name = signal.Signals(signum).name
    logger.warning(f"Received signal {signal_name} ({signum}), initiating graceful shutdown...")
    _shutdown_requested = True

# Register signal handlers
signal.signal(signal.SIGTERM, signal_handler)
signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGHUP, signal_handler)

###############################################################################
# RPC Endpoint Management with Enhanced Failover and Diagnostics
###############################################################################
def get_rpc_endpoint() -> dict:
    """Get the next working RPC endpoint with automatic failover and diagnostics."""
    global _rpc_endpoint_index, _failed_rpc_endpoints, _rpc_endpoint_success_count
    
    # Count of working endpoints (not yet exhausted)
    working_endpoints = [e for e in RPC_ENDPOINTS if _failed_rpc_endpoints.get(e["url"], {}).get("count", 0) < 3]
    
    # If all endpoints exhausted, reset ALL and try again
    if not working_endpoints:
        logger.warning("‚ö†Ô∏è  ALL RPC ENDPOINTS EXHAUSTED - Resetting failure counters and retrying")
        _failed_rpc_endpoints.clear()
        _rpc_endpoint_index = 0
        working_endpoints = RPC_ENDPOINTS
    
    # Sort by priority and success count
    working_endpoints.sort(key=lambda e: (e["priority"], -_rpc_endpoint_success_count.get(e["url"], 0)))
    
    # Get next endpoint (round-robin through working list)
    endpoint = working_endpoints[_rpc_endpoint_index % len(working_endpoints)]
    _rpc_endpoint_index = (_rpc_endpoint_index + 1) % len(working_endpoints)
    
    failure_info = _failed_rpc_endpoints.get(endpoint["url"], {})
    failure_count = failure_info.get("count", 0)
    
    if failure_count > 0:
        logger.debug(
            f"Selected RPC: {endpoint['name']} ({failure_count}/3 failures)\n"
            f"  URL: {endpoint['url']}\n"
            f"  Last error: {failure_info.get('last_error', 'N/A')}"
        )
    else:
        logger.debug(f"Selected RPC: {endpoint['name']} (healthy) - {endpoint['url']}")
    
    return endpoint

def mark_rpc_failed(endpoint_url: str, error: str = "", error_code: str = ""):
    """Mark an RPC endpoint as failed with detailed diagnostics."""
    global _failed_rpc_endpoints
    
    if endpoint_url not in _failed_rpc_endpoints:
        _failed_rpc_endpoints[endpoint_url] = {"count": 0, "last_error": "", "first_failure": None, "errors": []}
    
    _failed_rpc_endpoints[endpoint_url]["count"] += 1
    _failed_rpc_endpoints[endpoint_url]["last_error"] = error
    _failed_rpc_endpoints[endpoint_url]["last_failure_time"] = datetime.now(timezone.utc).isoformat()
    
    # Track error history (last 5 errors)
    if len(_failed_rpc_endpoints[endpoint_url]["errors"]) >= 5:
        _failed_rpc_endpoints[endpoint_url]["errors"].pop(0)
    _failed_rpc_endpoints[endpoint_url]["errors"].append({
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "error": error,
        "error_code": error_code
    })
    
    endpoint_name = next((e["name"] for e in RPC_ENDPOINTS if e["url"] == endpoint_url), "Unknown")
    failure_count = _failed_rpc_endpoints[endpoint_url]["count"]
    
    logger.warning(
        f"‚ö†Ô∏è  RPC ENDPOINT FAILED: {endpoint_name}\n"
        f"  Failure Count: {failure_count}/3\n"
        f"  URL: {endpoint_url}\n"
        f"  Error Code: {error_code or 'N/A'}\n"
        f"  Error Message: {error[:100] if error else 'N/A'}"
    )
    
    # If endpoint exhausted, log which endpoints are still available
    if failure_count >= 3:
        available = [e for e in RPC_ENDPOINTS if _failed_rpc_endpoints.get(e["url"], {}).get("count", 0) < 3]
        if available:
            logger.info(f"   {endpoint_name} exhausted. Available endpoints: {', '.join(e['name'] for e in available)}")
        else:
            logger.error(f"   ALL RPC ENDPOINTS EXHAUSTED! Will reset and retry.")

def reset_rpc_endpoint(endpoint_url: str):
    """Reset RPC endpoint failure count after successful use."""
    global _failed_rpc_endpoints, _rpc_endpoint_success_count
    
    if endpoint_url in _failed_rpc_endpoints:
        old_count = _failed_rpc_endpoints[endpoint_url]["count"]
        _failed_rpc_endpoints[endpoint_url]["count"] = 0
        endpoint_name = next((e["name"] for e in RPC_ENDPOINTS if e["url"] == endpoint_url), "Unknown")
        if old_count > 0:
            logger.info(f"‚úÖ {endpoint_name} recovered - failure counter reset (was {old_count}/3)")
    
    # Track success count
    if endpoint_url not in _rpc_endpoint_success_count:
        _rpc_endpoint_success_count[endpoint_url] = 0
    _rpc_endpoint_success_count[endpoint_url] += 1

def get_rpc_health_report() -> str:
    """Generate a health report of all RPC endpoints."""
    report = "\n" + "="*70 + "\n"
    report += "RPC ENDPOINT HEALTH REPORT\n"
    report += "="*70 + "\n"
    
    for endpoint in RPC_ENDPOINTS:
        url = endpoint["url"]
        failure_info = _failed_rpc_endpoints.get(url, {})
        failures = failure_info.get("count", 0)
        successes = _rpc_endpoint_success_count.get(url, 0)
        status = "‚úÖ Healthy" if failures < 3 else "‚ùå Exhausted"
        
        report += f"{endpoint['name']:<30} {status:<12} F:{failures}/3 S:{successes}\n"
        
        if failures > 0:
            last_error = failure_info.get("last_error", "N/A")
            last_time = failure_info.get("last_failure_time", "N/A")
            report += f"  Last error: {last_error[:60]}\n"
            report += f"  Last failure: {last_time}\n"
    
    report += "="*70 + "\n"
    return report

###############################################################################
# Data Fetching (Latest)
###############################################################################
def fetch_latest_btcusd_hourly(hours: int = 168, api_timeout: int = 30) -> pd.DataFrame:
    """Fetch recent BTC/USD hourly data for prediction."""
    logger.info(f"Fetching latest {hours}h BTC/USD data from Tiingo...")
    tkey = os.getenv("TIINGO_API_KEY", "").strip()
    if not tkey:
        logger.warning("TIINGO_API_KEY not set; generating synthetic.")
        # Synthetic for latest
        base = 40000.0
        rng = np.random.default_rng(int(time.time()))
        returns = rng.normal(0, 0.002, size=hours)
        prices = []
        current = base
        for r in returns:
            current *= math.exp(r)
            prices.append(current)
        start = datetime.now(tz=timezone.utc) - pd.Timedelta(hours=hours)
        timestamps = [start + pd.Timedelta(hours=i) for i in range(hours)]
        df = pd.DataFrame({"timestamp": timestamps, "close": prices})
        return df
    try:
        # Adjust start date to fetch more data
        start_date = (datetime.now(timezone.utc) - pd.Timedelta(hours=hours)).strftime("%Y-%m-%d")
        url = "https://api.tiingo.com/tiingo/crypto/prices"
        params = {
            "tickers": "btcusd",
            "startDate": start_date,
            "resampleFreq": "1hour",
            "token": tkey,
        }
        r = requests.get(url, params=params, timeout=api_timeout)
        r.raise_for_status()
        data = r.json()
        price_data = data[0].get("priceData", [])
        if len(price_data) < hours * 0.5:  # If less than 50% of expected
            logger.warning(f"Tiingo returned only {len(price_data)} rows, expected ~{hours}; using synthetic fallback.")
            # Fallback
            base = 40000.0
            rng = np.random.default_rng(int(time.time()))
            returns = rng.normal(0, 0.002, size=hours)
            prices = []
            current = base
            for r in returns:
                current *= math.exp(r)
                prices.append(current)
            start = datetime.now(tz=timezone.utc) - pd.Timedelta(hours=hours)
            timestamps = [start + pd.Timedelta(hours=i) for i in range(hours)]
            df = pd.DataFrame({"timestamp": timestamps, "close": prices})
            return df
        rows = []
        for item in price_data[-hours:]:  # Take last hours
            dt = datetime.fromisoformat(item["date"].replace("Z", "+00:00"))
            rows.append((dt, float(item["close"])))
        df = pd.DataFrame(rows, columns=["timestamp", "close"]).drop_duplicates("timestamp").sort_values("timestamp")
        logger.info(f"Fetched {len(df)} latest rows from Tiingo")
        return df
    except Exception as e:
        logger.warning(f"Tiingo fetch failed ({e}); using synthetic.")
        # Same as above
        base = 40000.0
        rng = np.random.default_rng(int(time.time()))
        returns = rng.normal(0, 0.002, size=hours)
        prices = []
        current = base
        for r in returns:
            current *= math.exp(r)
            prices.append(current)
        start = datetime.now(tz=timezone.utc) - pd.Timedelta(hours=hours)
        timestamps = [start + pd.Timedelta(hours=i) for i in range(hours)]
        df = pd.DataFrame({"timestamp": timestamps, "close": prices})
        return df

###############################################################################
# Feature Engineering (Same as training)
###############################################################################
def generate_features(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df["log_price"] = np.log(df["close"])
    df["ret_1h"] = df["log_price"].diff(1)
    df["ret_24h"] = df["log_price"].diff(24)
    df["ma_24h"] = df["close"].rolling(24).mean()
    df["ma_72h"] = df["close"].rolling(72).mean()
    df["vol_24h"] = df["ret_1h"].rolling(24).std()
    df["price_pos_24h"] = df["close"] / df["ma_24h"] - 1.0
    df["price_pos_72h"] = df["close"] / df["ma_72h"] - 1.0
    df["ma_ratio_72_24"] = df["ma_72h"] / df["ma_24h"] - 1.0
    df["exp_vol_ratio"] = df["vol_24h"].rolling(24).mean() / (df["vol_24h"] + 1e-8) - 1.0
    df = df.dropna().reset_index(drop=True)
    return df

###############################################################################
# Prediction
###############################################################################
def predict_forward_log_return(model, x_live: np.ndarray) -> float:
    pred = float(model.predict(x_live)[0])
    logger.info(f"Predicted 168h log-return: {pred:.8f}")
    return pred

###############################################################################
# Model Validation
###############################################################################
def validate_model(model_path: str, feature_count: int) -> bool:
    """Validate that loaded model is fitted and functional."""
    import pickle
    try:
        with open(model_path, "rb") as f:
            model = pickle.load(f)
        logger.info(f"‚úÖ Model loaded from {model_path}")
    except Exception as e:
        logger.error(f"‚ùå Failed to load model: {e}")
        return False
    
    # Check if fitted
    try:
        if not hasattr(model, 'n_features_in_'):
            logger.error("‚ùå Model not fitted (missing n_features_in_ attribute)")
            logger.error("   This usually means train.py was not run or model was saved without fitting.")
            logger.error("   Fix: Run 'python train.py' to train and save a fitted model.")
            return False
        if model.n_features_in_ != feature_count:
            logger.error(f"‚ùå Feature count mismatch: model expects {model.n_features_in_}, got {feature_count}")
            logger.error("   This usually means features.json is outdated.")
            logger.error("   Fix: Run 'python train.py' to regenerate features.json.")
            return False
        logger.info(f"‚úÖ Model is fitted with n_features_in_={model.n_features_in_}")
    except Exception as e:
        logger.error(f"‚ùå Model fitted-state verification failed: {e}")
        return False
    
    # Test prediction on dummy input
    try:
        dummy_input = np.zeros((1, feature_count))
        test_pred = model.predict(dummy_input)
        logger.info(f"‚úÖ Model test prediction passed: {float(test_pred[0]):.8f}")
    except Exception as e:
        logger.error(f"‚ùå Model test prediction failed: {e}")
        logger.error("   This usually means model.pkl is corrupted or incompatible.")
        logger.error("   Fix: Run 'python train.py' to retrain and save a fresh model.")
        return False
    
    return True

###############################################################################
# Get Account Sequence with Enhanced RPC Handling & Nonce/Sequence Diagnostics
###############################################################################
def get_account_sequence(wallet: str) -> Tuple[int, str, bool]:
    """
    Query account sequence from Allora network with RPC failover and validation.
    Returns: (sequence, endpoint_name, success_flag)
    Explicitly handles nonce/sequence mismatches and retries across endpoints.
    """
    cli = shutil.which("allorad") or shutil.which("allora")
    if not cli:
        logger.error("‚ùå Allora CLI not found in PATH")
        return 0, "none", False
    
    max_attempts = 3  # Try multiple endpoints before giving up
    attempts_made = 0
    last_error = ""
    
    for attempt in range(max_attempts):
        attempts_made += 1
        rpc_endpoint = get_rpc_endpoint()
        
        logger.debug(f"Account sequence query attempt {attempt + 1}/{max_attempts} via {rpc_endpoint['name']}")
        
        cmd = [cli, "query", "auth", "account", wallet, 
               "--node", rpc_endpoint["url"],
               "--output", "json"]
        
        try:
            proc = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            
            if proc.returncode == 0:
                # Validate response is JSON, not HTML error
                is_valid, data = validate_json_response(proc.stdout, f"for account {wallet}")
                if not is_valid:
                    error = "Invalid JSON response (likely HTML error page)"
                    logger.warning(f"‚ö†Ô∏è  Attempt {attempt + 1}: {error}")
                    mark_rpc_failed(rpc_endpoint["url"], error, error_code="INVALID_JSON")
                    last_error = error
                    continue
                
                # Extract sequence with validation
                try:
                    sequence = int(data["account"]["value"]["sequence"])
                    logger.info(f"‚úÖ Account sequence retrieved: {sequence} from {rpc_endpoint['name']}")
                    reset_rpc_endpoint(rpc_endpoint["url"])
                    return sequence, rpc_endpoint["name"], True
                except (KeyError, ValueError, TypeError) as e:
                    error = f"Malformed sequence response: {e}"
                    logger.warning(f"‚ö†Ô∏è  Attempt {attempt + 1}: {error}")
                    mark_rpc_failed(rpc_endpoint["url"], error, error_code="MALFORMED_RESPONSE")
                    last_error = error
                    continue
            else:
                error_msg = proc.stderr.strip()
                
                # Explicit error classification
                if "account does not exist" in error_msg.lower():
                    logger.error(f"‚ùå Account {wallet} does not exist on-chain")
                    logger.error("   Possible causes:")
                    logger.error("   1. Wallet address is incorrect")
                    logger.error("   2. Account has not been initialized yet")
                    logger.error("   3. Account was deleted or not funded")
                    return 0, rpc_endpoint["name"], False
                elif "nonce" in error_msg.lower() or "sequence" in error_msg.lower():
                    logger.warning(f"‚ö†Ô∏è  Nonce/Sequence mismatch detected: {error_msg[:80]}")
                    mark_rpc_failed(rpc_endpoint["url"], error_msg, error_code="SEQUENCE_MISMATCH")
                    last_error = error_msg
                    # Try next endpoint
                    continue
                else:
                    logger.warning(f"‚ö†Ô∏è  Query failed (attempt {attempt + 1}/{max_attempts}) on {rpc_endpoint['name']}: {error_msg[:100]}")
                    mark_rpc_failed(rpc_endpoint["url"], error_msg, error_code="QUERY_FAILED")
                    last_error = error_msg
                    # Try next endpoint
                    continue
                    
        except subprocess.TimeoutExpired:
            error = "Query timeout (30s)"
            logger.warning(f"‚ö†Ô∏è  Attempt {attempt + 1}: {error}")
            mark_rpc_failed(rpc_endpoint["url"], error, error_code="TIMEOUT")
            last_error = error
            continue
        except Exception as e:
            error = f"Unexpected error: {e}"
            logger.warning(f"‚ö†Ô∏è  Attempt {attempt + 1}: {error}")
            mark_rpc_failed(rpc_endpoint["url"], error, error_code="EXCEPTION")
            last_error = error
            continue
    
    # All attempts failed
    logger.error(f"‚ùå Cannot get account sequence after {attempts_made} attempts across RPC endpoints")
    logger.error(f"   Last error: {last_error}")
    logger.warning(get_rpc_health_report())
    return 0, "all_failed", False

###############################################################################
# Get Unfulfilled Nonce with Enhanced RPC Handling & Diagnostics
###############################################################################
def get_unfulfilled_nonce(topic_id: int) -> Tuple[int, str, bool]:
    """
    Query unfulfilled nonces from Allora network with RPC failover and validation.
    Returns: (nonce, endpoint_name, success_flag)
    Explicitly handles no-nonce scenarios and retries across endpoints.
    """
    cli = shutil.which("allorad") or shutil.which("allora")
    if not cli:
        logger.error("‚ùå Allora CLI not found in PATH")
        return 0, "none", False
    
    max_attempts = 3  # Try multiple endpoints
    attempts_made = 0
    last_error = ""
    
    for attempt in range(max_attempts):
        attempts_made += 1
        rpc_endpoint = get_rpc_endpoint()
        logger.debug(f"Unfulfilled nonce query attempt {attempt + 1}/{max_attempts} via {rpc_endpoint['name']}")
        
        cmd = [cli, "query", "emissions", "unfulfilled-worker-nonces", str(topic_id),
               "--node", rpc_endpoint["url"],
               "--output", "json"]
        
        try:
            proc = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            if proc.returncode == 0:
                # Validate response is JSON
                is_valid, data = validate_json_response(proc.stdout, f"for unfulfilled nonces (topic {topic_id})")
                if not is_valid:
                    error = "Invalid JSON response"
                    logger.warning(f"‚ö†Ô∏è  Attempt {attempt + 1}: {error}")
                    mark_rpc_failed(rpc_endpoint["url"], error, error_code="INVALID_JSON")
                    last_error = error
                    continue
                
                try:
                    nonces_data = data.get("nonces", {}).get("nonces", [])
                    nonces = [int(item["block_height"]) for item in nonces_data]
                    
                    if not nonces:
                        logger.info(f"‚ÑπÔ∏è  No unfulfilled nonces found for topic {topic_id} (all caught up)")
                        reset_rpc_endpoint(rpc_endpoint["url"])
                        return 0, rpc_endpoint["name"], False  # Not an error, just nothing to do
                    
                    logger.debug(f"‚úÖ Found {len(nonces)} unfulfilled nonces: {nonces[:5]}{'...' if len(nonces) > 5 else ''}")
                    
                    # Filter out nonces already submitted
                    wallet = os.getenv("ALLORA_WALLET_ADDR", "").strip()
                    if not wallet:
                        logger.error("‚ùå ALLORA_WALLET_ADDR not set")
                        return 0, rpc_endpoint["name"], False
                    
                    filtered_nonces = []
                    for nonce in nonces[:10]:  # Check first 10 to avoid excessive queries
                        try:
                            cmd_check = [cli, "query", "emissions", "worker-latest-inference", 
                                        str(topic_id), wallet,
                                        "--node", rpc_endpoint["url"],
                                        "--output", "json"]
                            proc_check = subprocess.run(cmd_check, capture_output=True, text=True, timeout=15)
                            
                            if proc_check.returncode == 0:
                                is_valid_check, data_check = validate_json_response(proc_check.stdout)
                                if is_valid_check:
                                    latest_bh = int(data_check.get("latest_inference", {}).get("block_height", 0))
                                    if latest_bh != nonce:
                                        filtered_nonces.append(nonce)
                                else:
                                    filtered_nonces.append(nonce)
                            else:
                                filtered_nonces.append(nonce)
                        except Exception as e:
                            logger.debug(f"  Nonce {nonce} check skipped: {e}")
                            filtered_nonces.append(nonce)
                    
                    if filtered_nonces:
                        selected_nonce = min(filtered_nonces)
                        logger.info(f"üéØ Selected nonce for submission: block_height={selected_nonce} via {rpc_endpoint['name']}")
                        reset_rpc_endpoint(rpc_endpoint["url"])
                        return selected_nonce, rpc_endpoint["name"], True
                    else:
                        logger.warning(f"‚ö†Ô∏è  All unfulfilled nonces already submitted by worker {wallet}")
                        reset_rpc_endpoint(rpc_endpoint["url"])
                        return 0, rpc_endpoint["name"], False
                
                except (KeyError, ValueError, TypeError) as e:
                    error = f"Malformed nonce response: {e}"
                    logger.warning(f"‚ö†Ô∏è  Attempt {attempt + 1}: {error}")
                    mark_rpc_failed(rpc_endpoint["url"], error, error_code="MALFORMED_RESPONSE")
                    last_error = error
                    continue
            else:
                error_msg = proc.stderr.strip()
                logger.warning(f"‚ö†Ô∏è  Query failed (attempt {attempt + 1}) on {rpc_endpoint['name']}: {error_msg[:100]}")
                mark_rpc_failed(rpc_endpoint["url"], error_msg, error_code="QUERY_FAILED")
                last_error = error_msg
                continue
                
        except subprocess.TimeoutExpired:
            error = "Query timeout (30s)"
            logger.warning(f"‚ö†Ô∏è  Attempt {attempt + 1}: {error}")
            mark_rpc_failed(rpc_endpoint["url"], error, error_code="TIMEOUT")
            last_error = error
            continue
        except Exception as e:
            error = f"Unexpected error: {e}"
            logger.warning(f"‚ö†Ô∏è  Attempt {attempt + 1}: {error}")
            mark_rpc_failed(rpc_endpoint["url"], error, error_code="EXCEPTION")
            last_error = error
            continue
    
    logger.error(f"‚ùå Cannot get unfulfilled nonce after {attempts_made} attempts across RPC endpoints")
    logger.error(f"   Last error: {last_error}")
    return 0, "all_failed", False

###############################################################################
# Validate Transaction On-Chain with Enhanced Response Handling
###############################################################################
def validate_transaction_on_chain(tx_hash: str, rpc_endpoint: dict) -> bool:
    """Verify that a transaction actually landed on-chain with response validation."""
    try:
        logger.debug(f"Validating transaction {tx_hash} on-chain via {rpc_endpoint['name']}...")
        
        cmd = ["curl", "-s", "-m", "30", f"{rpc_endpoint['url']}cosmos/tx/v1beta1/txs/{tx_hash}"]
        proc = subprocess.run(cmd, capture_output=True, text=True, timeout=35)
        
        if proc.returncode == 0:
            # Validate response is JSON, not HTML error
            is_valid, resp = validate_json_response(proc.stdout, f"for transaction {tx_hash}")
            if not is_valid:
                logger.warning(f"‚ö†Ô∏è  Invalid response when validating transaction {tx_hash}")
                return False
            
            # Check if transaction succeeded
            if resp.get("code") == 0 or "tx" in resp:
                logger.info(f"‚úÖ Transaction {tx_hash} confirmed on-chain via {rpc_endpoint['name']}")
                reset_rpc_endpoint(rpc_endpoint["url"])
                return True
            else:
                logger.warning(f"‚ö†Ô∏è  Transaction {tx_hash} not confirmed: {resp.get('message', 'unknown error')}")
                return False
        else:
            logger.debug(f"Could not validate transaction {tx_hash}: {proc.stderr}")
            return False
    except Exception as e:
        logger.debug(f"Error validating transaction: {e}")
        return False

###############################################################################
# CSV Submission Logging with Enhanced Tracking and Verification
###############################################################################
def log_submission_to_csv(timestamp: str, topic_id: int, prediction: float, worker: str, 
                          block_height: int, proof: dict, signature: str, status: str, 
                          tx_hash: str = "", rpc_endpoint: str = "unknown", 
                          attempts: int = 1, on_chain_verified: bool = False, 
                          error_details: str = ""):
    """Log submission to CSV with comprehensive tracking and verification status."""
    csv_path = "submission_log.csv"
    
    record = {
        "timestamp": timestamp,
        "topic_id": str(topic_id),
        "prediction": str(prediction),
        "worker": worker,
        "block_height": str(block_height),
        "proof": json.dumps(proof) if proof else "",
        "signature": signature,
        "status": status,
        "tx_hash": tx_hash or "",
        "rpc_endpoint": rpc_endpoint,
        "attempts": str(attempts),
        "on_chain_verified": "yes" if on_chain_verified else "no",
        "error_details": error_details,
    }
    
    try:
        import csv
        
        # Check if file exists to determine if we need headers
        file_exists = os.path.exists(csv_path)
        
        with open(csv_path, "a", newline="") as f:
            fieldnames = [
                "timestamp", "topic_id", "prediction", "worker", "block_height", 
                "proof", "signature", "status", "tx_hash", "rpc_endpoint",
                "attempts", "on_chain_verified", "error_details"
            ]
            w = csv.DictWriter(f, fieldnames=fieldnames)
            
            # Write header if file is new
            if not file_exists:
                w.writeheader()
            
            w.writerow(record)
        
        status_icon = "‚úÖ" if status.startswith("success") else "‚ö†Ô∏è" if status == "pending" else "‚ùå"
        logger.info(
            f"{status_icon} CSV Logged: status={status}, tx={tx_hash[:8] if tx_hash else 'none'}..., "
            f"on_chain={on_chain_verified}, attempts={attempts}, rpc={rpc_endpoint}"
        )
    except Exception as e:
        logger.error(f"‚ùå Failed to log to CSV: {e}")
        logger.debug(f"Traceback: {traceback.format_exc()}")

def log_heartbeat_to_csv(status: str = "alive"):
    """Log hourly heartbeat entry to track daemon liveness."""
    csv_path = "submission_log.csv"
    timestamp = datetime.now(timezone.utc).isoformat()
    
    try:
        import csv
        
        file_exists = os.path.exists(csv_path)
        
        with open(csv_path, "a", newline="") as f:
            fieldnames = [
                "timestamp", "topic_id", "prediction", "worker", "block_height", 
                "proof", "signature", "status", "tx_hash", "rpc_endpoint",
                "attempts", "on_chain_verified", "error_details"
            ]
            w = csv.DictWriter(f, fieldnames=fieldnames)
            
            if not file_exists:
                w.writeheader()
            
            w.writerow({
                "timestamp": timestamp,
                "topic_id": "HEARTBEAT",
                "prediction": "",
                "worker": "",
                "block_height": "",
                "proof": "",
                "signature": "",
                "status": f"heartbeat_{status}",
                "tx_hash": "",
                "rpc_endpoint": "",
                "attempts": "",
                "on_chain_verified": "",
                "error_details": "",
            })
        
        logger.info(f"üíì Heartbeat logged at {timestamp}")
    except Exception as e:
        logger.debug(f"Failed to log heartbeat: {e}")

###############################################################################
# Submission with Enhanced RPC Failover, Retry Logic, and CSV Logging
###############################################################################
def submit_prediction(value: float, topic_id: int, dry_run: bool = False) -> bool:
    """Submit prediction with RPC failover, retry logic, and comprehensive CSV logging."""
    global _submission_attempt_count
    _submission_attempt_count = 0
    
    timestamp = datetime.now(timezone.utc).isoformat()
    wallet = os.getenv("ALLORA_WALLET_ADDR", "").strip()
    
    if not wallet:
        logger.error("‚ùå ALLORA_WALLET_ADDR not set")
        return False
    
    logger.info(f"üöÄ LEADERBOARD SUBMISSION: Preparing prediction for topic {topic_id}")
    
    # Get unfulfilled nonce
    block_height, nonce_rpc, nonce_success = get_unfulfilled_nonce(topic_id)
    if not nonce_success or block_height == 0:
        logger.warning("‚ö†Ô∏è  No unfulfilled nonce available - submission skipped")
        log_submission_to_csv(
            timestamp=timestamp,
            topic_id=topic_id,
            prediction=value,
            worker=wallet,
            block_height=0,
            proof={},
            signature="",
            status="skipped_no_nonce",
            tx_hash="",
            rpc_endpoint=nonce_rpc,
            attempts=1,
            on_chain_verified=False,
            error_details="No unfulfilled nonces available"
        )
        return False
    
    logger.info(f"üìä Prediction value: {value:.10f}")
    logger.info(f"üìç Block height: {block_height}")

    # Get account sequence
    sequence, seq_rpc, seq_success = get_account_sequence(wallet)
    if not seq_success or sequence == 0:
        logger.error("‚ùå Cannot get account sequence - submission aborted")
        log_submission_to_csv(
            timestamp=timestamp,
            topic_id=topic_id,
            prediction=value,
            worker=wallet,
            block_height=block_height,
            proof={},
            signature="",
            status="failed_no_sequence",
            tx_hash="",
            rpc_endpoint=seq_rpc,
            attempts=1,
            on_chain_verified=False,
            error_details="Failed to retrieve account sequence"
        )
        return False
    
    logger.debug(f"Account sequence: {sequence}")

    # Create wallet for signing
    mnemonic = os.getenv("MNEMONIC", "").strip()
    if not mnemonic:
        logger.error("‚ùå MNEMONIC not set")
        return False
    
    try:
        wallet_obj = LocalWallet.from_mnemonic(mnemonic)
    except Exception as e:
        logger.error(f"‚ùå Failed to create wallet: {e}")
        return False

    # Create protobuf bundle
    try:
        inference = InputInference(
            topic_id=topic_id,
            block_height=block_height,
            inferer=wallet,
            value=str(value),
            extra_data=b"",
            proof=""
        )
        bundle = InputInferenceForecastBundle(inference=inference)
        bundle_bytes = bundle.SerializeToString()
        digest = hashlib.sha256(bundle_bytes).digest()
        sig = wallet_obj._private_key.sign_digest(digest)
        bundle_signature = base64.b64encode(sig).decode()
    except Exception as e:
        logger.error(f"‚ùå Failed to create bundle/signature: {e}")
        return False

    worker_data = {
        "worker": wallet,
        "nonce": {"block_height": block_height},
        "topic_id": topic_id,
        "inference_forecasts_bundle": {
            "inference": {
                "topic_id": topic_id,
                "block_height": block_height,
                "inferer": wallet,
                "value": str(value),
                "extra_data": "",
                "proof": ""
            }
        },
        "inferences_forecasts_bundle_signature": bundle_signature,
        "pubkey": "036ebdd2e91e40fe2e78200c788bf442cf2504a94a0b3eb328dcbda826d526d372"
    }

    cli = shutil.which("allorad") or shutil.which("allora")
    if not cli:
        logger.error("‚ùå Allora CLI not found")
        return False

    # Submission with retry
    success = False
    tx_hash = None
    used_rpc = None
    attempts_count = 0
    on_chain_verified = False
    
    for attempt in range(_max_submission_retries):
        attempts_count = attempt + 1
        _submission_attempt_count = attempt + 1
        rpc_endpoint = get_rpc_endpoint()
        used_rpc = rpc_endpoint
        
        logger.info(f"üì§ Attempt {_submission_attempt_count}/{_max_submission_retries}: Submitting via {rpc_endpoint['name']}")
        
        cmd = [cli, "tx", "emissions", "insert-worker-payload",
               wallet,
               json.dumps(worker_data),
               "--from", wallet,
               "--yes",
               "--keyring-backend", "test",
               "--node", rpc_endpoint["url"],
               "--chain-id", "allora-testnet-1",
               "--fees", "2500000uallo",
               "--broadcast-mode", "sync",
               "--gas", "250000",
               "--sequence", str(sequence),
               "--output", "json"]
        if dry_run:
            cmd.append("--dry-run")

        try:
            proc = subprocess.run(cmd, capture_output=True, text=True, timeout=120)
            
            if proc.returncode == 0:
                is_valid, resp = validate_json_response(proc.stdout)
                if not is_valid:
                    error = "Invalid JSON response"
                    logger.warning(f"‚ö†Ô∏è  Attempt {attempt + 1}: {error}")
                    mark_rpc_failed(rpc_endpoint["url"], error, error_code="INVALID_JSON")
                    if attempt < _max_submission_retries - 1:
                        continue
                    break
                
                if resp.get("code") == 0:
                    tx_hash = resp.get('txhash', '')
                    logger.info(f"‚úÖ LEADERBOARD SUBMISSION ACCEPTED")
                    logger.info(f"   TX: {tx_hash}")
                    logger.info(f"   Block: {block_height}, Prediction: {value:.10f}")
                    
                    reset_rpc_endpoint(rpc_endpoint["url"])
                    
                    # Attempt on-chain verification
                    if validate_transaction_on_chain(tx_hash, rpc_endpoint):
                        logger.info(f"üéâ CONFIRMED: Submission on-chain!")
                        on_chain_verified = True
                        success = True
                    else:
                        logger.info("‚è≥ Pending confirmation (will verify in next cycle)")
                        success = True
                    break
                else:
                    error_msg = resp.get('raw_log', 'Unknown')
                    logger.error(f"‚ùå Rejected: {error_msg}")
                    mark_rpc_failed(rpc_endpoint["url"], error_msg, error_code="TX_REJECTED")
                    if attempt < _max_submission_retries - 1:
                        continue
                    break
            else:
                cli_error = proc.stderr.strip()
                logger.error(f"‚ùå CLI failed: {cli_error[:80]}")
                mark_rpc_failed(rpc_endpoint["url"], cli_error, error_code="CLI_ERROR")
                if attempt < _max_submission_retries - 1:
                    continue
                break
        except subprocess.TimeoutExpired:
            error = "Submission timeout (120s)"
            logger.error(f"‚ùå {error}")
            mark_rpc_failed(rpc_endpoint["url"], error, error_code="TIMEOUT")
            if attempt < _max_submission_retries - 1:
                continue
            break
        except Exception as e:
            error = f"Exception: {e}"
            logger.error(f"‚ùå {error}")
            mark_rpc_failed(rpc_endpoint["url"], str(e), error_code="EXCEPTION")
            if attempt < _max_submission_retries - 1:
                continue
            break

    # Log result to CSV - ALWAYS log, never silently fail
    final_status = "success_confirmed" if success else "failed_submission"
    error_details = "" if success else "All submission attempts failed or were rejected"
    
    log_submission_to_csv(
        timestamp=timestamp,
        topic_id=topic_id,
        prediction=value,
        worker=wallet,
        block_height=block_height,
        proof=worker_data["inference_forecasts_bundle"],
        signature=bundle_signature,
        status=final_status,
        tx_hash=tx_hash or "",
        rpc_endpoint=used_rpc["name"] if used_rpc else "unknown",
        attempts=attempts_count,
        on_chain_verified=on_chain_verified,
        error_details=error_details
    )

    return success

def main():
    parser = argparse.ArgumentParser(description="Submit BTC/USD 7-day log-return prediction.")
    parser.add_argument("--model", type=str, default="model.pkl", help="Path to trained model.")
    parser.add_argument("--features", type=str, default="features.json", help="Path to feature columns.")
    parser.add_argument("--topic-id", type=int, default=int(os.getenv("TOPIC_ID", "67")), help="Allora topic ID.")
    parser.add_argument("--dry-run", action="store_true", help="Simulate submission without sending.")
    parser.add_argument("--once", action="store_true", help="Run once and exit (replaces --continuous when not set).")
    parser.add_argument("--continuous", action="store_true", help="Run in continuous mode, submitting every hour.")
    parser.add_argument("--daemon", action="store_true", help="Run as permanent daemon (until Dec 15, 2025).")
    args = parser.parse_args()
    
    # Validate critical files exist before entering continuous mode
    if not os.path.exists(args.model):
        logger.error(f"‚ùå CRITICAL: {args.model} not found. Run 'python train.py' first.")
        sys.exit(1)
    if not os.path.exists(args.features):
        logger.error(f"‚ùå CRITICAL: {args.features} not found. Run 'python train.py' first.")
        sys.exit(1)
    
    # Validate environment
    required_env = ["ALLORA_WALLET_ADDR", "MNEMONIC", "TOPIC_ID"]
    missing = [k for k in required_env if not os.getenv(k)]
    if missing:
        logger.error(f"‚ùå Missing environment variables: {', '.join(missing)}")
        sys.exit(1)

    if args.daemon or args.continuous:
        run_daemon(args)
    else:
        # Single run mode
        exit_code = main_once(args)
        sys.exit(0 if exit_code else 1)

def run_daemon(args):
    """
    Run as a long-lived daemon until December 15, 2025.
    Handles all exceptions, never silently fails, includes hourly heartbeat.
    """
    global _shutdown_requested, _last_heartbeat_hour
    
    interval = int(os.getenv("SUBMISSION_INTERVAL", "3600"))  # 1 hour default
    competition_end = datetime(2025, 12, 15, 0, 0, 0, tzinfo=timezone.utc)
    
    logger.info("=" * 80)
    logger.info("üöÄ DAEMON MODE STARTED (REFACTORED WITH ENHANCED RPC HANDLING)")
    logger.info(f"   Model: {args.model}")
    logger.info(f"   Features: {args.features}")
    logger.info(f"   Topic ID: {args.topic_id}")
    logger.info(f"   Submission Interval: {interval}s ({interval/3600:.1f}h)")
    logger.info(f"   Competition End: {competition_end.isoformat()}")
    logger.info(f"   Current Time: {datetime.now(timezone.utc).isoformat()}")
    logger.info("=" * 80)
    
    cycle_count = 0
    _last_heartbeat_hour = None
    
    while not _shutdown_requested:
        cycle_count += 1
        cycle_start = datetime.now(timezone.utc)
        
        # Check if competition has ended
        if cycle_start >= competition_end:
            logger.info(f"‚è∞ Competition end date ({competition_end.isoformat()}) reached. Shutting down.")
            break
        
        # Hourly heartbeat and RPC health report (separate from submission attempts)
        now_hour = cycle_start.replace(minute=0, second=0, microsecond=0)
        if _last_heartbeat_hour != now_hour:
            logger.info(f"üíì HEARTBEAT - Daemon alive at {cycle_start.isoformat()}")
            log_heartbeat_to_csv("alive")
            logger.info(get_rpc_health_report())
            _last_heartbeat_hour = now_hour
        
        try:
            logger.info(f"\n{'='*80}")
            logger.info(f"SUBMISSION CYCLE #{cycle_count} - {cycle_start.isoformat()}")
            logger.info(f"{'='*80}")
            
            success = main_once(args)
            
            if success:
                logger.info("‚úÖ Submission cycle completed successfully")
            else:
                logger.warning("‚ö†Ô∏è  Submission cycle completed without successful submission (may be skipped/no nonce)")
        
        except Exception as e:
            # CRITICAL: Never silently fail
            logger.error(f"‚ùå UNHANDLED EXCEPTION IN SUBMISSION CYCLE #{cycle_count}")
            logger.error(f"   Exception: {type(e).__name__}: {str(e)}")
            logger.error("   Full traceback:")
            for line in traceback.format_exc().split('\n'):
                if line.strip():
                    logger.error(f"   {line}")
            log_heartbeat_to_csv("error")
            # Continue to next cycle instead of crashing
        
        try:
            if not _shutdown_requested:
                logger.info(f"Sleeping for {interval}s until next submission cycle...")
                time.sleep(interval)
        except KeyboardInterrupt:
            logger.warning("Interrupted during sleep, proceeding to next cycle")
            pass
    
    logger.info("=" * 80)
    logger.info("üõë DAEMON SHUTDOWN COMPLETE")
    logger.info(f"   Total Cycles: {cycle_count}")
    logger.info(f"   Final Time: {datetime.now(timezone.utc).isoformat()}")
    logger.info(f"   Final RPC Health: {get_rpc_health_report()}")
    logger.info("=" * 80)

def main_once(args):
    """Execute a single submission cycle with comprehensive error handling."""
    try:
        # Load and validate features first
        if not os.path.exists(args.features):
            logger.error(f"‚ùå Features file not found: {args.features}")
            logger.error("   Run 'python train.py' to generate features.json")
            return False
        
        with open(args.features, "r") as f:
            feature_cols = json.load(f)
        logger.info(f"‚úÖ Loaded {len(feature_cols)} feature columns")
        
        # Validate model before using it (CRITICAL FOR DAEMON)
        if not validate_model(args.model, len(feature_cols)):
            logger.error(f"‚ùå CRITICAL: Model validation failed. Cannot proceed with submission.")
            logger.error("   Run 'python train.py' to train a new model.")
            logger.error("   Daemon will retry in next cycle.")
            return False
        
        # Load model (we know it's valid now)
        import pickle
        with open(args.model, "rb") as f:
            model = pickle.load(f)
        logger.debug("Model loaded and validated")

        # Fetch latest data with error handling
        try:
            raw = fetch_latest_btcusd_hourly()
            logger.debug(f"Fetched {len(raw)} raw price records")
        except Exception as e:
            logger.error(f"‚ùå Failed to fetch BTC/USD data: {e}")
            logger.error("   Retrying in next cycle")
            for line in traceback.format_exc().split('\n'):
                if line.strip():
                    logger.debug(line)
            return False
        
        # Feature engineering with error handling
        try:
            feats = generate_features(raw)
            if len(feats) == 0:
                logger.error("‚ùå No feature data available after feature engineering")
                logger.error("   Raw data insufficient or feature generation failed")
                logger.error("   Retrying in next cycle")
                return False
            logger.debug(f"Generated features for {len(feats)} records")
        except Exception as e:
            logger.error(f"‚ùå Feature engineering failed: {e}")
            logger.error("   Retrying in next cycle")
            for line in traceback.format_exc().split('\n'):
                if line.strip():
                    logger.debug(line)
            return False
        
        # Prepare input for prediction
        try:
            latest = feats.iloc[-1]
            x_live = latest[feature_cols].values.reshape(1, -1)
            logger.debug(f"Prepared prediction input (shape: {x_live.shape})")
        except KeyError as e:
            logger.error(f"‚ùå Missing feature column: {e}")
            logger.error("   Feature mismatch between features.json and current data.")
            logger.error("   Run 'python train.py' to regenerate features.json")
            logger.error("   Retrying in next cycle")
            return False
        except Exception as e:
            logger.error(f"‚ùå Error preparing prediction input: {e}")
            logger.error("   Retrying in next cycle")
            for line in traceback.format_exc().split('\n'):
                if line.strip():
                    logger.debug(line)
            return False

        # Predict with error handling
        try:
            pred = predict_forward_log_return(model, x_live)
            logger.debug(f"Prediction computed: {pred:.8f}")
        except Exception as e:
            logger.error(f"‚ùå Prediction failed: {e}")
            logger.error("   Model may be corrupted or incompatible")
            logger.error("   Retrying in next cycle")
            for line in traceback.format_exc().split('\n'):
                if line.strip():
                    logger.debug(line)
            return False

        # Submit with error handling
        try:
            success = submit_prediction(pred, args.topic_id, dry_run=args.dry_run)
            if success:
                logger.info(f"‚úÖ Submission status: SUCCESS")
            else:
                logger.info(f"‚ö†Ô∏è  Submission status: skipped or failed (may be no unfulfilled nonce)")
            return success
        except Exception as e:
            logger.error(f"‚ùå Submission failed with exception: {e}")
            logger.error("   Retrying in next cycle")
            for line in traceback.format_exc().split('\n'):
                if line.strip():
                    logger.debug(line)
            return False
    
    except Exception as e:
        # Final catch-all to ensure no silent failures
        logger.error(f"‚ùå UNHANDLED EXCEPTION in main_once: {type(e).__name__}: {str(e)}")
        logger.error("   This should not happen - indicates a bug in cycle logic")
        for line in traceback.format_exc().split('\n'):
            if line.strip():
                logger.error(f"   {line}")
        return False

if __name__ == "__main__":
    main()
